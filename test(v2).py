# -*- coding: utf-8 -*-
"""test(v2).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_WM47-0tJnGBD2_Q-ZYBOJKO6oMhJ8Sp
"""

import numpy as n
import torch
import torchvision.models as models
import torch.nn as nn
from torch.utils.tensorboard import SummaryWriter
from MyNetwork import SkipCombination
from MyNetwork import SkipCombination_DownSample
from MyNetwork import MyResNet34
import numpy as np
import pandas as pd
import io
from google.colab import files
import os
import shelve
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import label_binarize
from sklearn.metrics import roc_curve, auc
from sklearn.multiclass import OneVsRestClassifier
from itertools import cycle
my_model = torch.load('FinalModel')

# Read test data

with shelve.open('TestData', flag='r') as file:
    testimages = file['images'] # N, H, W
    testlabels = file['labels']

testimages = np.expand_dims(testimages, axis=1) # N, C, H, W

prediction_accuracy = 0
y_score = []
for i in range(100):
  result = my_model(torch.tensor(testimages[(100*i):(100*(i+1)),:,:,:],device='cuda:0', dtype=torch.float32))
  y_test = torch.tensor(testlabels[(100*i):(100*(i+1))],device='cuda:0', dtype=torch.int64)
  # prediction score
  y_score.append(result.cpu().detach().numpy())
  # prediction label
  labels_pred = torch.argmax(result, dim=1)
  # count number of correct prediction
  correct = (y_test == labels_pred).float()
  accuracy = correct.sum() / correct.numel()
  # overall prediction accuracy
  prediction_accuracy += accuracy/100

# concatenate all the prediction score (used to calculate the ROC curve)
y_score = np.concatenate(y_score)

# binarize (one-hot encoding) the outcome
y_roc = label_binarize(testlabels, classes=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
n_classes = y_roc.shape[1]

# create empty dict() to store fpr, tpr and roc-auc values
fpr = dict()
tpr = dict()
roc_auc = dict()
lw=2
for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_roc[:, i], y_score[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])
colors = cycle(['blue', 'red', 'green',"aqua", "darkorange", "cornflowerblue","deeppink",
                "navy","yellow", "black"])
for i, color in zip(range(n_classes), colors):
    plt.plot(fpr[i], tpr[i], color=color, lw=2,
             label='ROC curve of class {0} (area = {1:0.2f})'
             ''.format(i, roc_auc[i]))

# visualize the roc curve with different colors
plt.plot([0, 1], [0, 1], 'k--', lw=lw)
plt.xlim([-0.05, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for the MNIST Testing Set Prediction')
plt.legend(loc="lower right")
plt.show()